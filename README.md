<p align="center">
  <h1 align="center">ğŸ” RAG Document Intelligence</h1>
  <p align="center">
    <img src="https://github.com/salehA13/rag-document-intelligence/actions/workflows/ci.yml/badge.svg" alt="CI">
  </p>
  <p align="center">
    Production-grade Retrieval-Augmented Generation system with hybrid semantic + keyword search and re-ranking
  </p>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white" alt="Python"></a>
  <a href="https://fastapi.tiangolo.com"><img src="https://img.shields.io/badge/FastAPI-0.115-009688?logo=fastapi&logoColor=white" alt="FastAPI"></a>
  <a href="https://www.langchain.com"><img src="https://img.shields.io/badge/LangChain-0.3-1C3C3C?logo=langchain&logoColor=white" alt="LangChain"></a>
  <a href="https://www.trychroma.com"><img src="https://img.shields.io/badge/ChromaDB-1.0-FF6F00" alt="ChromaDB"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT"></a>
</p>

---

## The Problem

Traditional keyword search fails when users phrase questions differently from the source text. Pure vector search misses exact term matches. **Neither approach alone is reliable for document Q&A.**

RAG Document Intelligence solves this with a **hybrid retrieval pipeline** that combines both strategies and fuses the results using Reciprocal Rank Fusion (RRF) â€” the same technique used by production search engines â€” to deliver consistently relevant answers grounded in your documents.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG Document Intelligence                          â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   ğŸ“„ Ingestion       â”‚         â”‚   ğŸ” Query Pipeline              â”‚  â”‚
â”‚   â”‚                      â”‚         â”‚                                  â”‚  â”‚
â”‚   â”‚   PDF Upload/Dir     â”‚         â”‚   User Question                  â”‚  â”‚
â”‚   â”‚        â”‚             â”‚         â”‚        â”‚                         â”‚  â”‚
â”‚   â”‚   PyPDF Loader       â”‚         â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚   â”‚        â”‚             â”‚         â”‚        â–¼          â–¼             â”‚  â”‚
â”‚   â”‚   Recursive Text     â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚   â”‚   Splitter           â”‚         â”‚   â”‚Semantic â”‚ â”‚ Keyword â”‚     â”‚  â”‚
â”‚   â”‚   (1000 char chunks  â”‚         â”‚   â”‚ Search  â”‚ â”‚ Search  â”‚     â”‚  â”‚
â”‚   â”‚    200 overlap)      â”‚         â”‚   â”‚(ChromaDB)â”‚ â”‚ (BM25)  â”‚     â”‚  â”‚
â”‚   â”‚        â”‚             â”‚         â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚   â”‚   OpenAI Embeddings  â”‚         â”‚        â”‚           â”‚           â”‚  â”‚
â”‚   â”‚   (text-embedding-   â”‚         â”‚        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚   â”‚    3-small)          â”‚         â”‚              â–¼                  â”‚  â”‚
â”‚   â”‚        â”‚             â”‚         â”‚   Reciprocal Rank Fusion        â”‚  â”‚
â”‚   â”‚   ChromaDB + SHA-256 â”‚         â”‚   (k=60, per Cormack 2009)     â”‚  â”‚
â”‚   â”‚   Dedup Store        â”‚         â”‚              â”‚                  â”‚  â”‚
â”‚   â”‚                      â”‚         â”‚       Re-ranked Results         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚              â”‚                  â”‚  â”‚
â”‚                                    â”‚       GPT-4o-mini + Context     â”‚  â”‚
â”‚                                    â”‚              â”‚                  â”‚  â”‚
â”‚                                    â”‚    Answer + Source Citations     â”‚  â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  ğŸŒ FastAPI REST API              ğŸ“Š Streamlit Frontend         â”‚  â”‚
â”‚   â”‚  POST /ask    POST /upload        Interactive Document Q&A      â”‚  â”‚
â”‚   â”‚  POST /search POST /ingest        PDF Upload & Ingestion        â”‚  â”‚
â”‚   â”‚  GET  /stats  GET  /health        Source Attribution View       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Features

| Feature | Description |
|---------|-------------|
| **Hybrid Search** | Combines dense vector retrieval (ChromaDB) with sparse BM25 keyword matching for robust recall |
| **Reciprocal Rank Fusion** | Merges ranked lists using RRF (`1/(k+rank)`) â€” outperforms single-strategy retrieval without tuning |
| **Source Attribution** | Every answer cites the exact source filename and page number |
| **Content Deduplication** | SHA-256 content hashing prevents duplicate embeddings across re-ingestions |
| **REST API** | Full FastAPI backend with OpenAPI docs, file upload, and typed request/response models |
| **Interactive UI** | Streamlit frontend for drag-and-drop PDF upload, Q&A, and retrieval-only search |
| **CLI Ingestion** | Batch-process entire directories of PDFs from the command line |
| **Configurable** | All parameters (chunk size, overlap, top-k, models) via environment variables |

---

## Why Hybrid Search + RRF?

| Strategy | Strength | Weakness |
|----------|----------|----------|
| **Semantic only** | Understands meaning, synonyms, paraphrasing | Misses exact terms, acronyms, proper nouns |
| **Keyword only** | Precise term matching, fast | No understanding of meaning or context |
| **Hybrid + RRF** | **Best of both** â€” high recall and precision | Slightly more compute (negligible in practice) |

RRF is parameter-free (only `k=60` constant) and consistently improves retrieval quality without requiring training data or score normalization across methods.

---

## Tech Stack

| Component | Technology |
|-----------|-----------|
| LLM | OpenAI GPT-4o-mini |
| Embeddings | OpenAI `text-embedding-3-small` (1536 dims) |
| Vector Store | ChromaDB (persistent, local) |
| Framework | LangChain 0.3 |
| API | FastAPI + Uvicorn |
| Frontend | Streamlit |
| Keyword Search | BM25Okapi (`rank-bm25`) |
| Re-ranking | Reciprocal Rank Fusion |
| PDF Processing | PyPDF |
| Config | Pydantic Settings |

---

## Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/salehA13/rag-document-intelligence.git
cd rag-document-intelligence

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

### 2. Configure

```bash
cp .env.example .env
# Add your OpenAI API key to .env
```

### 3. Ingest Documents

```bash
# Ingest the sample PDFs
python ingest.py ./docs

# Ingest a single file
python ingest.py path/to/paper.pdf

# Check store stats
python ingest.py --stats .
```

### 4. Start the API

```bash
uvicorn src.api.server:app --reload --port 8000
```

Interactive docs at: **http://localhost:8000/docs**

### 5. Launch the UI (optional)

```bash
streamlit run src/frontend/app.py
```

---

## API Reference

### `GET /health`

Health check.

```json
{ "status": "healthy", "version": "1.0.0" }
```

### `GET /stats`

Vector store statistics.

```json
{ "total_documents": 142, "persist_dir": "./data/chroma" }
```

### `POST /ask`

Full RAG pipeline â€” retrieve, re-rank, generate answer with citations.

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the self-attention mechanism?",
    "top_k": 10,
    "rerank_k": 5
  }'
```

```json
{
  "answer": "The self-attention mechanism allows each position in a sequence to attend to all other positions...",
  "sources": [
    { "filename": "transformer_survey.pdf", "page": 3 },
    { "filename": "transformer_survey.pdf", "page": 7 }
  ],
  "num_sources": 5
}
```

### `POST /search`

Retrieval-only â€” returns ranked document chunks without LLM generation.

```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"question": "attention mechanisms", "top_k": 10, "rerank_k": 5}'
```

### `POST /upload`

Upload and ingest a single PDF.

```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@paper.pdf"
```

### `POST /ingest`

Batch-ingest all PDFs from a directory.

```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{"directory": "./docs"}'
```

---

## Testing

```bash
pytest tests/ -v
```

All **13 tests** cover the ingestion pipeline, hybrid search logic, RRF correctness, and API endpoints:

```
tests/test_loader.py   â€” chunking, metadata enrichment, edge cases
tests/test_search.py   â€” BM25, RRF merging, tokenization
tests/test_api.py      â€” health, stats, upload validation, error handling
```

---

## Project Structure

```
rag-document-intelligence/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                  # Centralized settings (pydantic-settings)
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ loader.py              # PDF loading & recursive text chunking
â”‚   â”‚   â””â”€â”€ embedder.py            # ChromaDB vector store + SHA-256 dedup
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ hybrid.py              # Hybrid search: semantic + BM25 + RRF
â”‚   â”‚   â””â”€â”€ qa.py                  # QA chain with source attribution
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ models.py              # Typed Pydantic request/response schemas
â”‚   â”‚   â””â”€â”€ server.py              # FastAPI application + CORS
â”‚   â””â”€â”€ frontend/
â”‚       â””â”€â”€ app.py                 # Streamlit interactive UI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader.py             # Ingestion pipeline tests
â”‚   â”œâ”€â”€ test_search.py             # Search & RRF tests
â”‚   â””â”€â”€ test_api.py                # API endpoint tests
â”œâ”€â”€ docs/                          # Sample PDFs for demo
â”œâ”€â”€ ingest.py                      # CLI ingestion tool
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Configuration

All settings are managed via environment variables (`.env` file) with sensible defaults:

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | â€” | Your OpenAI API key (required) |
| `OPENAI_MODEL` | `gpt-4o-mini` | LLM for answer generation |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | Embedding model |
| `CHROMA_PERSIST_DIR` | `./data/chroma` | ChromaDB storage path |
| `CHUNK_SIZE` | `1000` | Characters per chunk |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `TOP_K` | `10` | Retrieval candidates |
| `RERANK_TOP_K` | `5` | Final results after RRF |

---

## License

[MIT](LICENSE)

---

Built by [Saleh](https://github.com/salehA13)
